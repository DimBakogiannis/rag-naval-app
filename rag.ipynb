{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Youtube RAG application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Add the link to the YouTube video you're going to use.\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=3qHkcs3kG44\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the model\n",
    "Define the LLM model that you will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model by asking a simple question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 14, 'total_tokens': 22, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_71b02749fa', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model returns an `AIMessage` object that contains the response. To get the actual text answer, we can use an [output parser](https://python.langchain.com/docs/modules/model_io/output_parsers/) to convert the message into a usable format.\n",
    "\n",
    "In this case, we'll use the `StrOutputParser` to extract the response as a plain string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser\n",
    "chain.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribing the YouTube Video\n",
    "\n",
    "The context we want to send the model comes from a YouTube video. Let's download the video and transcribe it using [OpenAI's Whisper](https://openai.com/research/whisper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import whisper\n",
    "from pytube import YouTube\n",
    "\n",
    "\n",
    "# Run for the first download of the video\n",
    "if not os.path.exists(\"transcription.txt\"):\n",
    "    youtube = YouTube(YOUTUBE_VIDEO)\n",
    "    audio = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "    whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        file = audio.download(output_path=tmpdir)\n",
    "        transcription = whisper_model.transcribe(file, fp16=False)[\"text\"].strip()\n",
    "\n",
    "        with open(\"transcription.txt\", \"w\") as file:\n",
    "            file.write(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you have downloaded the transcript and it works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Two, one. Boom, all right, well, thank you very much for doing this, man. I really appreciate it. I've been absorbing your information and listening to you talk for quite a while now, so it's great to actually meet you. Thanks for having me.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"transcription.txt\") as file:\n",
    "    transcription = file.read()\n",
    "\n",
    "transcription[:241]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt templates\n",
    "To help the model generate better responses, we combine context with the user’s question.\n",
    "[Prompt templates](https://python.langchain.com/docs/modules/model_io/prompts/quick_start) make it easy to define, manage, and reuse these prompts in a consistent way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Answer the question based on the context below. If you can't \n",
      "answer the question, reply \"I don't know\".\n",
      "\n",
      "Context: In the episode, Naval talks about the power of leverage through code and media.\n",
      "\n",
      "Question: What type of leverage does Naval mention?\n",
      "\n",
      "Answer: Naval mentions leverage through code and media.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "formatted_prompt = prompt.format(\n",
    "    context=\"In the episode, Naval talks about the power of leverage through code and media.\",\n",
    "    question=\"What type of leverage does Naval mention?\"\n",
    ")\n",
    "\n",
    "print(formatted_prompt)\n",
    "\n",
    "\n",
    "# Chain the prompt with the model and the output parser\n",
    "chain = prompt | model | parser\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"context\": \"In the episode, Naval talks about the power of leverage through code and media.\",\n",
    "    \"question\": \"What type of leverage does Naval mention?\"\n",
    "})\n",
    "\n",
    "print(f\"Answer: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the transcription\n",
    "\n",
    "Because the full transcription is too large to be provided as context for the model, a common approach is to divide it into smaller, manageable segments. \n",
    "\n",
    "This way, the model can focus only on the most relevant parts when responding to a specific question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10000 characters of the document:\n",
      "--------------------------------------------------\n",
      "Two, one. Boom, all right, well, thank you very much for doing this, man. I really appreciate it. I've been absorbing your information and listening to you talk for quite a while now, so it's great to actually meet you. Thanks for having me. My pleasure, my pleasure. You are one of the rare guys that is, you're a big investor, you're deep in the tech world, but yet you seem to have a very balanced perspective in terms of how to live life, as opposed to not just be entirely focused on success and financial success and tech investing, but rather how to live your life in a happy way. That's a, it's a, it's not balance. Yeah, you know, I think the reason why people like hearing me is because like if it's like, if you go to a circus and you see a bear, right, that's kind of interesting, but not that much. If you see a unicycle, that's interesting, but you see a bear on a unicycle, that's really interesting, right? So when you combine things, you're not supposed to combine. Right. People get interested. It's a Bruce Lee, right, striking thoughts, philosophy, plus martial arts. And I think it's because of some level, all humans are broad. We're all multivariate, but we get summarized in pithy ways in our lives, and at some deep level, we know that's not true, right? Every human basically is capable of every experience and every thought. Your UFC comedian, commentator, podcaster, but you're also more than that. You're also father, lover, thinker, et cetera. So I like the model of life that the ancients had, the Greeks, the Romans, right, where you would start out, and when you're young, you're just like going to school, then you're going to war, then you're running a business, then you're supposed to serve in the Senate or the government, then you become a philosopher, this sort of this arc to life, where you try your hand at everything. And as one of my friends says, specialization is for insects, right? So everyone should just be able to do everything. And so I don't believe in this model anymore of trying to focus your life down on one thing. You've got one life, just do everything you're going to do. I couldn't agree more. And I think that sometimes people find certain success in whatever the endeavor is, and then they think that that is their niche, and they stick with it, and they never change. And they almost have out of fear. Well, it's hard because there's a analogy around mountain climbing. Like if you find a mountain, and you start climbing, and you spend your whole life climbing it, and you get, say, two-thirds of the way. And then you see the peak is like way up there, but you're two-thirds of the way up. You're still really high up. But now to go the rest of the way, you're going to have to go back down to the bottom and look for another path. Nobody wants to do that. People don't want to start over. And it's the nature of later in life that you just don't have the time. So it's very painful to go back down and look for a new path, but that may be the best thing to do. And that's why when you look at the greatest artists and creators, they have this ability to start over that nobody else does. Like Elon will be called an idiot and start over, doing something brand new that he supposedly is not qualified for. Or when Madonna or Paul Simon are you to come out a new album, they're existing fans, usually hate it, because they've adopted a completely new style that they've learned somewhere else. A lot of times they'll just miss completely. So you have to be willing to be a fool and have that beginner's mind and go back to the beginning to start over. And if you're not doing that, you're just getting older. Yeah, I mean, I don't even know if it's willing to be a fool. It's just to me that the most exciting thing is to try to get better at something, to learn that thing. So it's really exciting when you just have incremental progress in something that you're completely new to. Yeah, I live for the aha moment, that moment when you connect two things together, that you hadn't connected to before, and it fits nicely and solidly, and it helps form a steel framework of understanding in your mind that you can then hang other ideas off of. That's what I live for. It's a curiosity fulfilled. And it's what little children do, too. My little son is always asking, why, why, why, why? And I always try and answer him and half the times I realize, actually, I don't really understand why. I just have a memorized answer for you, but that's not really understanding. Yeah, those are weird conversations, right? When you're talking to your kids and you say, look, the reality is, I don't know a lot of things. Yes. I just memorize a lot of things. And there's certain things that you just can't know. Yeah, you realize that you have answers for a few things that you've thought through. Then you sort of have cover-ups, like track doors. Don't go here. This is just a cover-up. I don't really know the answer to what the meaning of life is or how we got here. Yes. And then you've got a whole bunch of memorized stuff, because a lot of your intelligence these days just the external brain pack of civilization. I know it's out there. I know the answers are out there. I know how to look them up and I've memorized some of them. And I kind of understand how money works in the Federal Reserve, Princeton, and what this government thing is, but not really. Right. So not good enough to teach it in university. Yeah. I think people do that with almost everything in life these days in terms of like a one page, a one sheet, like a brief summary of what the explanation for what this very complex subject might be. TLDR, right? Yes. Don't give me the lecture. Give me the book. Don't give me the book. Give me the blog post. Don't give me the blog post. Give me the tweet. Don't give me the tweet. I just already know. Yeah. I got really fascinated by the way you read, because I thought there was something wrong with me by doing that. But you don't really just read a book to completion. You read, and then you pick something else up, and you just kind of go based on your whims, whatever you're interested in. Well, I was raised by my, I was raised by a single mom in New York, and she used the local library as a daycare center, because it was a very tough neighborhood. And so she would basically say, when you get back from school, go straight to the library and don't come out, until I pick you up late at night. So I used to basically live in the library, and I read everything. I read every magazine, I read every pictograph, I read every book, I read every map. I just read our stuff to read. I just read everything. So I got over this idea of that reading a large number of books are reading a book to completion as a vanity metric, because really when people are putting a photos on Twitter, Instagram, and look at my pile of books that I'm reading, it's a show off thing, it's a signaling thing. And the reality is I would rather read the best hundred books over and over again until I absorb them, rather than read all the books. Right. Because your brain has finite information, in finite space, you get enough advice that all cancels to zero. There's a lot of nonsense in books out there too. So I don't read any more to complete books. I read to satisfy my genuine intellectual curiosity. And it can be anything. It could be nonsense, it could be history, it could be fiction, it could be science, it could be sci-fi. These days, it's mostly sci-fi philosophy science, because that's just what I'm interested in. But I will read for understanding. So a really good book, I will flip through. I won't actually read it consecutive in order, and I won't even just even finish it. I'm looking for ideas, things that I don't understand. And when I find something really interesting, I'll reflect on it, I'll research it. And then when I'm bored of it, I'll drop it, or I'll flip to another book. Thanks to electronic books, I've got 50, 70 books open at any time in my Kindle, or I books, and I'm just bouncing around between them. It's also a little bit of a defense mechanism to how in modern society, we get too much information too quickly, and so attention spans are very low. So you get Twitter, you get Instagram, you get Facebook, you just used to being bombarded with information. So you can take that to, you can view that as a negative and be like, I have no attention span, or you could view that as a positive. I multitask really well, and I can dig really fast. I can, if I find a thread that's interesting, I can follow it through five social networks, through the web, through the libraries, through the books, and I can really get to the bottom of this thing very quickly. It's like the library of Alexandria that I can research in my disposal. So I no longer track books red, or even care about books red. It's about understanding concepts. Yeah, you brought up two awesome points. First of all, the social media aspect of books and basically anything. It's like, it's such a weird way to display your life, because you're displaying the best aspects of your life and some sort of a glass case. It's an unrealistic version of your life that you cultivate and you curate. And I'm as guilty of that as anybody. Everybody's guilty of it, I'm guilty of it too. Yeah, I mean, I pose with my dog every time I run. Yeah, we're always signaling. It's like rather than really looking at yourself, you're looking at how other people look at you. So it's like this one-removed mental image. And it's kind of a disease, because social media is making celebrities of all of us, and celebrities are the most miserable people in the world. Right, because they have this strong self image that gets built up, it gets built up by compliments. Every time somebody pays you or me a compliment, and we're like, oh, well, thank you. Then that builds up an image of who we are. And then one idiot comes along, one out of 10, one o...\n",
      "--------------------------------------------------\n",
      "\n",
      "Total document length: 145,626 characters\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"transcription.txt\")\n",
    "text_documents = loader.load()\n",
    "\n",
    "# Display first X characters \n",
    "print(f\"First 10000 characters of the document:\")\n",
    "print(\"-\" * 50)\n",
    "print(text_documents[0].page_content[:10000] + \"...\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\nTotal document length: {len(text_documents[0].page_content):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various techniques for breaking up a document. In this example, we’ll use a straightforward method that divides the text into fixed-size chunks. \n",
    "\n",
    "If you're curious about other strategies, check out the [Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/) guide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Two, one. Boom, all right, well, thank you very much for doing this, man. I really appreciate it.', metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"appreciate it. I've been absorbing your information and listening to you talk for quite a while\", metadata={'source': 'transcription.txt'})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "text_splitter.split_documents(text_documents)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = text_splitter.split_documents(text_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify most relevant chunks\n",
    "\n",
    "To answer a specific question, we need to pinpoint the most relevant parts of the transcription to pass to the model. This is where **embeddings** become useful.\n",
    "\n",
    "You can use the [Cohere's Embed Playground](https://dashboard.cohere.com/playground/embed) to visualize embeddings in two dimensions.\n",
    "\n",
    "Compute similarity between embeddings and then top-macthing will be used as context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length: 1536\n",
      "[0.02451018613922683, -0.011081933963308487, -0.0013095540911784872, -0.025292292210096785, -0.017471233364042407, 0.015541198646709315, -0.015806104970870788, -0.00674250860905136, 0.0016572442819246874, -0.03451357033119359]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embedded_query = embeddings.embed_query(\"What is the capital of France?\")\n",
    "\n",
    "print(f\"Embedding length: {len(embedded_query)}\")\n",
    "print(embedded_query[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how embeddings work, let's first generate the embeddings for two different sentences.\n",
    "\n",
    "We can now compute the similarity between the query and each of the two sentences. The closer the embeddings are, the more similar the sentences will be.\n",
    "\n",
    "We can use [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) to calculate the similarity between the query and each of the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8350762426685766, 0.7912412151713869)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sentence1 = embeddings.embed_query(\"Paris\")\n",
    "sentence2 = embeddings.embed_query(\"Athens\")\n",
    "\n",
    "\n",
    "query_sentence1_similarity = cosine_similarity([embedded_query], [sentence1])[0][0]\n",
    "query_sentence2_similarity = cosine_similarity([embedded_query], [sentence2])[0][0]\n",
    " \n",
    "query_sentence1_similarity, query_sentence2_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though both are capital cities, Paris is more semantically aligned with the question about France, so it scores slightly higher. \n",
    "\n",
    "The fact that both scores are relatively high shows that embeddings capture the semantic relationship (i.e., both are capitals), but the higher score for Paris confirms that it's more relevant to the specific query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a Vector Store\n",
    "\n",
    "We need an efficient way to store document chunks, their embeddings, and perform similarity searches at scale. To do this, we'll use a **vector store**.\n",
    "\n",
    "A vector store is a database of embeddings that specializes in fast similarity searches. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='wants. Apply some leverage, put your name on it. So you take the risks, but you gain the rewards,', metadata={'source': 'transcription.txt'}),\n",
       "  0.8367189667737388),\n",
       " (Document(page_content='because of the accountability that you have with your name because of leverage that you have', metadata={'source': 'transcription.txt'}),\n",
       "  0.8152599168369862),\n",
       " (Document(page_content='at was looking at businesses and figuring out the point of maximum leverage to actually create', metadata={'source': 'transcription.txt'}),\n",
       "  0.8102432310140415)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search_with_score(query=\"What is leverage?\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect the vector store to the chain\n",
    "\n",
    "To identify the most relevant parts of the transcription for the model, we’ll connect a vector store to our processing chain.\n",
    "\n",
    "This involves setting up a [Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/), which performs a similarity search on the vector store and returns the top-matching chunks.\n",
    "\n",
    "\n",
    "Since our prompt expects two inputs, \"context\" and \"question\", we need a way to supply both. To do this, we’ll use the [`RunnableParallel`](https://python.langchain.com/docs/expression_language/how_to/map) and [`RunnablePassthrough`](https://python.langchain.com/docs/expression_language/how_to/passthrough) classes from LangChain’s expression language. \n",
    "\n",
    "These utilities help us structure the inputs into a dictionary with the appropriate keys for the prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Leverage refers to the use of various methods or strategies to amplify one's potential gains or benefits. In the context provided, it suggests applying one's name or accountability to take risks in order to achieve greater rewards, particularly in business and technology. It indicates a strategic advantage that can lead to significant outcomes when properly utilized.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    {\"context\": vectorstore.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "chain.invoke(\"What is leverage?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Pinecone\n",
    "\n",
    "In-memory vector store is fine for small-scale examples.\n",
    "\n",
    "However, in real-world applications, you need a vector store that can scale with large datasets and support fast, efficient similarity searches.\n",
    "\n",
    " For this purpose, we'll use [Pinecone](https://www.pinecone.io/), a managed vector database designed for high-performance retrieval tasks.\n",
    "\n",
    "Create a Pinecone account, set up an index, get an API key, and set it as an environment variable `PINECONE_API_KEY`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "index_name = YOUR_INDEX_NAME\n",
    "\n",
    "pinecone = PineconeVectorStore.from_documents(\n",
    "    documents, embeddings, index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run a similarity search on pinecone to make sure everything works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"linger. So if you interpret the positive and everything very quickly, you let it go, right? You let it go much faster. Simple hacks get more sunlight, right? Learn to smile more, learn to hug more. These things actually release serotonin in reverse. They aren't just outward signals of being happy. They're actually feedback loops to being happy. Spend more time in nature. These are obvious. Watch your mind. Watch your mind all day long. Watch what it does. Not judge it. Not try to control it. But you can meditate 24-7. Meditation is not a sit down, close your eyes activity. Meditation is just basically watching your own thoughts like you would watch anything else in the outside world and say, why am I having that thought? Does that serve me anymore? Is that conditioning from when I was 10 years old? For example, getting ready for this podcast. You got ready? I didn't. Oh, good. I did. But I did. But I did. I couldn't help it. And what happened was the few days leading up to this, my\", metadata={'source': 'transcription.txt'})]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.similarity_search(\"What are the best tips to be happy?\")[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup the new chain using Pinecone as the vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The best tips to be happy mentioned in the context are:\\n\\n1. Interpret the positive quickly and let go of negativity.\\n2. Engage in simple hacks to boost your mood, such as getting more sunlight.\\n3. Learn to smile and hug more, as these actions can release serotonin and create feedback loops to happiness.\\n4. Spend more time in nature.\\n5. Watch your mind and observe your thoughts without judgment or trying to control them.\\n6. Practice continuous meditation by being aware of your thoughts and questioning their relevance and origins.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"context\": pinecone.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"What are the best tips to be happy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragapp_ytb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
