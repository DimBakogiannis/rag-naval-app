{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Youtube RAG application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Add the link to the YouTube video you're going to use.\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=3qHkcs3kG44\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the model\n",
    "Define the LLM model that you will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model by asking a simple question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 14, 'total_tokens': 22, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_71b02749fa', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model returns an `AIMessage` object that contains the response. To get the actual text answer, we can use an [output parser](https://python.langchain.com/docs/modules/model_io/output_parsers/) to convert the message into a usable format.\n",
    "\n",
    "In this case, we'll use the `StrOutputParser` to extract the response as a plain string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser\n",
    "chain.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribing the YouTube Video\n",
    "\n",
    "The context we want to send the model comes from a YouTube video. Let's download the video and transcribe it using [OpenAI's Whisper](https://openai.com/research/whisper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import whisper\n",
    "from pytube import YouTube\n",
    "\n",
    "\n",
    "# Run for the first download of the video\n",
    "if not os.path.exists(\"transcription.txt\"):\n",
    "    youtube = YouTube(YOUTUBE_VIDEO)\n",
    "    audio = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "    whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        file = audio.download(output_path=tmpdir)\n",
    "        transcription = whisper_model.transcribe(file, fp16=False)[\"text\"].strip()\n",
    "\n",
    "        with open(\"transcription.txt\", \"w\") as file:\n",
    "            file.write(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you have downloaded the transcript and it works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Two, one. Boom, all right, well, thank you very much for doing this, man. I really appreciate it. I've been absorbing your information and listening to you talk for quite a while now, so it's great to actually meet you. Thanks for having me.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"transcription.txt\") as file:\n",
    "    transcription = file.read()\n",
    "\n",
    "transcription[:241]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt templates\n",
    "To help the model generate better responses, we combine context with the user’s question.\n",
    "[Prompt templates](https://python.langchain.com/docs/modules/model_io/prompts/quick_start) make it easy to define, manage, and reuse these prompts in a consistent way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Answer the question based on the context below. If you can't \n",
      "answer the question, reply \"I don't know\".\n",
      "\n",
      "Context: In the episode, Naval talks about the power of leverage through code and media.\n",
      "\n",
      "Question: What type of leverage does Naval mention?\n",
      "\n",
      "Answer: Naval mentions leverage through code and media.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "formatted_prompt = prompt.format(\n",
    "    context=\"In the episode, Naval talks about the power of leverage through code and media.\",\n",
    "    question=\"What type of leverage does Naval mention?\"\n",
    ")\n",
    "\n",
    "print(formatted_prompt)\n",
    "\n",
    "\n",
    "# Chain the prompt with the model and the output parser\n",
    "chain = prompt | model | parser\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"context\": \"In the episode, Naval talks about the power of leverage through code and media.\",\n",
    "    \"question\": \"What type of leverage does Naval mention?\"\n",
    "})\n",
    "\n",
    "print(f\"Answer: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the transcription\n",
    "\n",
    "Because the full transcription is too large to be provided as context for the model, a common approach is to divide it into smaller, manageable segments. \n",
    "\n",
    "This way, the model can focus only on the most relevant parts when responding to a specific question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 2000 characters of the document:\n",
      "--------------------------------------------------\n",
      "Two, one. Boom, all right, well, thank you very much for doing this, man. I really appreciate it. I've been absorbing your information and listening to you talk for quite a while now, so it's great to actually meet you. Thanks for having me. My pleasure, my pleasure. You are one of the rare guys that is, you're a big investor, you're deep in the tech world, but yet you seem to have a very balanced perspective in terms of how to live life, as opposed to not just be entirely focused on success and financial success and tech investing, but rather how to live your life in a happy way. That's a, it's a, it's not balance. Yeah, you know, I think the reason why people like hearing me is because like if it's like, if you go to a circus and you see a bear, right, that's kind of interesting, but not that much. If you see a unicycle, that's interesting, but you see a bear on a unicycle, that's really interesting, right? So when you combine things, you're not supposed to combine. Right. People get interested. It's a Bruce Lee, right, striking thoughts, philosophy, plus martial arts. And I think it's because of some level, all humans are broad. We're all multivariate, but we get summarized in pithy ways in our lives, and at some deep level, we know that's not true, right? Every human basically is capable of every experience and every thought. Your UFC comedian, commentator, podcaster, but you're also more than that. You're also father, lover, thinker, et cetera. So I like the model of life that the ancients had, the Greeks, the Romans, right, where you would start out, and when you're young, you're just like going to school, then you're going to war, then you're running a business, then you're supposed to serve in the Senate or the government, then you become a philosopher, this sort of this arc to life, where you try your hand at everything. And as one of my friends says, specialization is for insects, right? So everyone should just be able to do everything. And so I don't beli...\n",
      "--------------------------------------------------\n",
      "\n",
      "Total document length: 145,626 characters\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"transcription.txt\")\n",
    "text_documents = loader.load()\n",
    "\n",
    "# Display first X characters \n",
    "print(f\"First 2000 characters of the document:\")\n",
    "print(\"-\" * 50)\n",
    "print(text_documents[0].page_content[:2000] + \"...\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\nTotal document length: {len(text_documents[0].page_content):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various techniques for breaking up a document. In this example, we’ll use a straightforward method that divides the text into fixed-size chunks. \n",
    "\n",
    "If you're curious about other strategies, check out the [Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/) guide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Two, one. Boom, all right, well, thank you very much for doing this, man. I really appreciate it.', metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"appreciate it. I've been absorbing your information and listening to you talk for quite a while\", metadata={'source': 'transcription.txt'})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "text_splitter.split_documents(text_documents)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = text_splitter.split_documents(text_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify most relevant chunks\n",
    "\n",
    "To answer a specific question, we need to pinpoint the most relevant parts of the transcription to pass to the model. This is where **embeddings** become useful.\n",
    "\n",
    "You can use the [Cohere's Embed Playground](https://dashboard.cohere.com/playground/embed) to visualize embeddings in two dimensions.\n",
    "\n",
    "Compute similarity between embeddings and then top-macthing will be used as context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length: 1536\n",
      "[0.02451018613922683, -0.011081933963308487, -0.0013095540911784872, -0.025292292210096785, -0.017471233364042407, 0.015541198646709315, -0.015806104970870788, -0.00674250860905136, 0.0016572442819246874, -0.03451357033119359]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embedded_query = embeddings.embed_query(\"What is the capital of France?\")\n",
    "\n",
    "print(f\"Embedding length: {len(embedded_query)}\")\n",
    "print(embedded_query[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how embeddings work, let's first generate the embeddings for two different sentences.\n",
    "\n",
    "We can now compute the similarity between the query and each of the two sentences. The closer the embeddings are, the more similar the sentences will be.\n",
    "\n",
    "We can use [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) to calculate the similarity between the query and each of the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8350762426685766, 0.7912412151713869)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sentence1 = embeddings.embed_query(\"Paris\")\n",
    "sentence2 = embeddings.embed_query(\"Athens\")\n",
    "\n",
    "\n",
    "query_sentence1_similarity = cosine_similarity([embedded_query], [sentence1])[0][0]\n",
    "query_sentence2_similarity = cosine_similarity([embedded_query], [sentence2])[0][0]\n",
    " \n",
    "query_sentence1_similarity, query_sentence2_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though both are capital cities, Paris is more semantically aligned with the question about France, so it scores slightly higher. \n",
    "\n",
    "The fact that both scores are relatively high shows that embeddings capture the semantic relationship (i.e., both are capitals), but the higher score for Paris confirms that it's more relevant to the specific query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a Vector Store\n",
    "\n",
    "We need an efficient way to store document chunks, their embeddings, and perform similarity searches at scale. To do this, we'll use a **vector store**.\n",
    "\n",
    "A vector store is a database of embeddings that specializes in fast similarity searches. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='wants. Apply some leverage, put your name on it. So you take the risks, but you gain the rewards,', metadata={'source': 'transcription.txt'}),\n",
       "  0.8367189667737388),\n",
       " (Document(page_content='because of the accountability that you have with your name because of leverage that you have', metadata={'source': 'transcription.txt'}),\n",
       "  0.8152599168369862),\n",
       " (Document(page_content='at was looking at businesses and figuring out the point of maximum leverage to actually create', metadata={'source': 'transcription.txt'}),\n",
       "  0.8102432310140415)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search_with_score(query=\"What is leverage?\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect the vector store to the chain\n",
    "\n",
    "To identify the most relevant parts of the transcription for the model, we’ll connect a vector store to our processing chain.\n",
    "\n",
    "This involves setting up a [Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/), which performs a similarity search on the vector store and returns the top-matching chunks.\n",
    "\n",
    "\n",
    "Since our prompt expects two inputs, \"context\" and \"question\", we need a way to supply both. To do this, we’ll use the [`RunnableParallel`](https://python.langchain.com/docs/expression_language/how_to/map) and [`RunnablePassthrough`](https://python.langchain.com/docs/expression_language/how_to/passthrough) classes from LangChain’s expression language. \n",
    "\n",
    "These utilities help us structure the inputs into a dictionary with the appropriate keys for the prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Leverage refers to the use of various methods or strategies to amplify one's potential gains or benefits. In the context provided, it suggests applying one's name or accountability to take risks in order to achieve greater rewards, particularly in business and technology. It indicates a strategic advantage that can lead to significant outcomes when properly utilized.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    {\"context\": vectorstore.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "chain.invoke(\"What is leverage?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Pinecone\n",
    "\n",
    "In-memory vector store is fine for small-scale examples.\n",
    "\n",
    "However, in real-world applications, you need a vector store that can scale with large datasets and support fast, efficient similarity searches.\n",
    "\n",
    " For this purpose, we'll use [Pinecone](https://www.pinecone.io/), a managed vector database designed for high-performance retrieval tasks.\n",
    "\n",
    "Create a Pinecone account, set up an index, get an API key, and set it as an environment variable `PINECONE_API_KEY`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "index_name = YOUR_INDEX_NAME\n",
    "\n",
    "pinecone = PineconeVectorStore.from_documents(\n",
    "    documents, embeddings, index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run a similarity search on pinecone to make sure everything works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"linger. So if you interpret the positive and everything very quickly, you let it go, right? You let it go much faster. Simple hacks get more sunlight, right? Learn to smile more, learn to hug more. These things actually release serotonin in reverse. They aren't just outward signals of being happy. They're actually feedback loops to being happy. Spend more time in nature. These are obvious. Watch your mind. Watch your mind all day long. Watch what it does. Not judge it. Not try to control it. But you can meditate 24-7. Meditation is not a sit down, close your eyes activity. Meditation is just basically watching your own thoughts like you would watch anything else in the outside world and say, why am I having that thought? Does that serve me anymore? Is that conditioning from when I was 10 years old? For example, getting ready for this podcast. You got ready? I didn't. Oh, good. I did. But I did. But I did. I couldn't help it. And what happened was the few days leading up to this, my\", metadata={'source': 'transcription.txt'})]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.similarity_search(\"What are the best tips to be happy?\")[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup the new chain using Pinecone as the vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The best tips to be happy mentioned in the context are:\\n\\n1. Interpret the positive quickly and let go of negativity.\\n2. Engage in simple hacks to boost your mood, such as getting more sunlight.\\n3. Learn to smile and hug more, as these actions can release serotonin and create feedback loops to happiness.\\n4. Spend more time in nature.\\n5. Watch your mind and observe your thoughts without judgment or trying to control them.\\n6. Practice continuous meditation by being aware of your thoughts and questioning their relevance and origins.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"context\": pinecone.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"What are the best tips to be happy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragapp_ytb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
